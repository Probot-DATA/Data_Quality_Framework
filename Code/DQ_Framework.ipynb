{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7280a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasets={\n",
    "           \"adult\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\adult\\adult.data\",\n",
    "           \"adult_test\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\adult\\adult.test\",\n",
    "           \"customer_churn\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\customer_churn\\WA_Fn-UseC_-Telco-Customer-Churn.csv\",\n",
    "           \"UCI_credit_card\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\UCI_credit_card\\UCI_Credit_Card.csv\",\n",
    "           \"yellow1\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\yellow_trip_data\\yellow_tripdata_2015-01.csv\",\n",
    "           \"yellow2\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\yellow_trip_data\\yellow_tripdata_2016-01.csv\",\n",
    "           \"yellow3\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\yellow_trip_data\\yellow_tripdata_2016-02.csv\",\n",
    "           \"yellow4\":r\"C:\\Users\\Prabhat\\Desktop\\Carear\\projects\\project-1\\data\\yellow_trip_data\\yellow_tripdata_2016-03.csv\"\n",
    "           }\n",
    "dataset_names=['yellow' , 'UCI_credit_card' , 'customer_churn' , 'adult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d39d6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>RatecodeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:30</td>\n",
       "      <td>2016-03-10 09:09:11</td>\n",
       "      <td>5</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-73.983955</td>\n",
       "      <td>40.737537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.989944</td>\n",
       "      <td>40.756554</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:31</td>\n",
       "      <td>2016-03-10 09:03:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-73.995209</td>\n",
       "      <td>40.749649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.980080</td>\n",
       "      <td>40.759201</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 08:58:45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-73.986633</td>\n",
       "      <td>40.761318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.979057</td>\n",
       "      <td>40.764980</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 09:00:38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-73.990707</td>\n",
       "      <td>40.745960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.981606</td>\n",
       "      <td>40.753780</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 09:05:56</td>\n",
       "      <td>2</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-73.985748</td>\n",
       "      <td>40.778610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.970840</td>\n",
       "      <td>40.768291</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0             2  2015-01-15 19:05:39   2015-01-15 19:23:42                1   \n",
       "1             1  2015-01-10 20:33:38   2015-01-10 20:53:28                1   \n",
       "2             1  2015-01-10 20:33:38   2015-01-10 20:43:41                1   \n",
       "3             1  2015-01-10 20:33:39   2015-01-10 20:35:31                1   \n",
       "4             1  2015-01-10 20:33:39   2015-01-10 20:52:58                1   \n",
       "...         ...                  ...                   ...              ...   \n",
       "79995         2  2016-03-10 08:52:30   2016-03-10 09:09:11                5   \n",
       "79996         2  2016-03-10 08:52:31   2016-03-10 09:03:29                1   \n",
       "79997         2  2016-03-10 08:52:32   2016-03-10 08:58:45                2   \n",
       "79998         2  2016-03-10 08:52:32   2016-03-10 09:00:38                1   \n",
       "79999         2  2016-03-10 08:52:32   2016-03-10 09:05:56                2   \n",
       "\n",
       "       trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0               1.59        -73.993896        40.750111         1.0   \n",
       "1               3.30        -74.001648        40.724243         1.0   \n",
       "2               1.80        -73.963341        40.802788         1.0   \n",
       "3               0.50        -74.009087        40.713818         1.0   \n",
       "4               3.00        -73.971176        40.762428         1.0   \n",
       "...              ...               ...              ...         ...   \n",
       "79995           1.91        -73.983955        40.737537         NaN   \n",
       "79996           1.38        -73.995209        40.749649         NaN   \n",
       "79997           0.65        -73.986633        40.761318         NaN   \n",
       "79998           0.87        -73.990707        40.745960         NaN   \n",
       "79999           1.31        -73.985748        40.778610         NaN   \n",
       "\n",
       "      store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                      N         -73.974785         40.750618             1   \n",
       "1                      N         -73.994415         40.759109             1   \n",
       "2                      N         -73.951820         40.824413             2   \n",
       "3                      N         -74.004326         40.719986             2   \n",
       "4                      N         -74.004181         40.742653             2   \n",
       "...                  ...                ...               ...           ...   \n",
       "79995                  N         -73.989944         40.756554             2   \n",
       "79996                  N         -73.980080         40.759201             1   \n",
       "79997                  N         -73.979057         40.764980             2   \n",
       "79998                  N         -73.981606         40.753780             1   \n",
       "79999                  N         -73.970840         40.768291             1   \n",
       "\n",
       "       fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             12.0    1.0      0.5        3.25           0.0   \n",
       "1             14.5    0.5      0.5        2.00           0.0   \n",
       "2              9.5    0.5      0.5        0.00           0.0   \n",
       "3              3.5    0.5      0.5        0.00           0.0   \n",
       "4             15.0    0.5      0.5        0.00           0.0   \n",
       "...            ...    ...      ...         ...           ...   \n",
       "79995         11.5    0.0      0.5        0.00           0.0   \n",
       "79996          8.5    0.0      0.5        0.00           0.0   \n",
       "79997          5.5    0.0      0.5        0.00           0.0   \n",
       "79998          6.5    0.0      0.5        1.46           0.0   \n",
       "79999          9.5    0.0      0.5        2.06           0.0   \n",
       "\n",
       "       improvement_surcharge  total_amount  RatecodeID  \n",
       "0                        0.3         17.05         NaN  \n",
       "1                        0.3         17.80         NaN  \n",
       "2                        0.3         10.80         NaN  \n",
       "3                        0.3          4.80         NaN  \n",
       "4                        0.3         16.30         NaN  \n",
       "...                      ...           ...         ...  \n",
       "79995                    0.3         12.30         1.0  \n",
       "79996                    0.3          9.30         1.0  \n",
       "79997                    0.3          6.30         1.0  \n",
       "79998                    0.3          8.76         1.0  \n",
       "79999                    0.3         12.36         1.0  \n",
       "\n",
       "[80000 rows x 20 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size=20000\n",
    "customer_churn=pd.read_csv(datasets[\"customer_churn\"])\n",
    "UCI_credit_card=pd.read_csv(datasets[\"UCI_credit_card\"])\n",
    "chunk1=pd.read_csv(datasets[\"yellow1\"],chunksize=chunk_size)\n",
    "yellow1 = next(chunk1) \n",
    "chunk2=pd.read_csv(datasets[\"yellow2\"],chunksize=chunk_size)\n",
    "yellow2 = next(chunk2) \n",
    "chunk3=pd.read_csv(datasets[\"yellow3\"],chunksize=chunk_size)\n",
    "yellow3 = next(chunk3) \n",
    "chunk4=pd.read_csv(datasets[\"yellow4\"],chunksize=chunk_size)\n",
    "yellow4 = next(chunk4)\n",
    "yellow= pd.concat([yellow1, yellow2, yellow3, yellow4], ignore_index=True)\n",
    "yellow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d4448e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>RatecodeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:30</td>\n",
       "      <td>2016-03-10 09:09:11</td>\n",
       "      <td>5</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-73.983955</td>\n",
       "      <td>40.737537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.989944</td>\n",
       "      <td>40.756554</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:31</td>\n",
       "      <td>2016-03-10 09:03:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-73.995209</td>\n",
       "      <td>40.749649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.980080</td>\n",
       "      <td>40.759201</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 08:58:45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-73.986633</td>\n",
       "      <td>40.761318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.979057</td>\n",
       "      <td>40.764980</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 09:00:38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-73.990707</td>\n",
       "      <td>40.745960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.981606</td>\n",
       "      <td>40.753780</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 08:52:32</td>\n",
       "      <td>2016-03-10 09:05:56</td>\n",
       "      <td>2</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-73.985748</td>\n",
       "      <td>40.778610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.970840</td>\n",
       "      <td>40.768291</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0             2  2015-01-15 19:05:39   2015-01-15 19:23:42                1   \n",
       "1             1  2015-01-10 20:33:38   2015-01-10 20:53:28                1   \n",
       "2             1  2015-01-10 20:33:38   2015-01-10 20:43:41                1   \n",
       "3             1  2015-01-10 20:33:39   2015-01-10 20:35:31                1   \n",
       "4             1  2015-01-10 20:33:39   2015-01-10 20:52:58                1   \n",
       "...         ...                  ...                   ...              ...   \n",
       "79995         2  2016-03-10 08:52:30   2016-03-10 09:09:11                5   \n",
       "79996         2  2016-03-10 08:52:31   2016-03-10 09:03:29                1   \n",
       "79997         2  2016-03-10 08:52:32   2016-03-10 08:58:45                2   \n",
       "79998         2  2016-03-10 08:52:32   2016-03-10 09:00:38                1   \n",
       "79999         2  2016-03-10 08:52:32   2016-03-10 09:05:56                2   \n",
       "\n",
       "       trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0               1.59        -73.993896        40.750111         1.0   \n",
       "1               3.30        -74.001648        40.724243         1.0   \n",
       "2               1.80        -73.963341        40.802788         1.0   \n",
       "3               0.50        -74.009087        40.713818         1.0   \n",
       "4               3.00        -73.971176        40.762428         1.0   \n",
       "...              ...               ...              ...         ...   \n",
       "79995           1.91        -73.983955        40.737537         NaN   \n",
       "79996           1.38        -73.995209        40.749649         NaN   \n",
       "79997           0.65        -73.986633        40.761318         NaN   \n",
       "79998           0.87        -73.990707        40.745960         NaN   \n",
       "79999           1.31        -73.985748        40.778610         NaN   \n",
       "\n",
       "      store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                      N         -73.974785         40.750618             1   \n",
       "1                      N         -73.994415         40.759109             1   \n",
       "2                      N         -73.951820         40.824413             2   \n",
       "3                      N         -74.004326         40.719986             2   \n",
       "4                      N         -74.004181         40.742653             2   \n",
       "...                  ...                ...               ...           ...   \n",
       "79995                  N         -73.989944         40.756554             2   \n",
       "79996                  N         -73.980080         40.759201             1   \n",
       "79997                  N         -73.979057         40.764980             2   \n",
       "79998                  N         -73.981606         40.753780             1   \n",
       "79999                  N         -73.970840         40.768291             1   \n",
       "\n",
       "       fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             12.0    1.0      0.5        3.25           0.0   \n",
       "1             14.5    0.5      0.5        2.00           0.0   \n",
       "2              9.5    0.5      0.5        0.00           0.0   \n",
       "3              3.5    0.5      0.5        0.00           0.0   \n",
       "4             15.0    0.5      0.5        0.00           0.0   \n",
       "...            ...    ...      ...         ...           ...   \n",
       "79995         11.5    0.0      0.5        0.00           0.0   \n",
       "79996          8.5    0.0      0.5        0.00           0.0   \n",
       "79997          5.5    0.0      0.5        0.00           0.0   \n",
       "79998          6.5    0.0      0.5        1.46           0.0   \n",
       "79999          9.5    0.0      0.5        2.06           0.0   \n",
       "\n",
       "       improvement_surcharge  total_amount  RatecodeID  \n",
       "0                        0.3         17.05         NaN  \n",
       "1                        0.3         17.80         NaN  \n",
       "2                        0.3         10.80         NaN  \n",
       "3                        0.3          4.80         NaN  \n",
       "4                        0.3         16.30         NaN  \n",
       "...                      ...           ...         ...  \n",
       "79995                    0.3         12.30         1.0  \n",
       "79996                    0.3          9.30         1.0  \n",
       "79997                    0.3          6.30         1.0  \n",
       "79998                    0.3          8.76         1.0  \n",
       "79999                    0.3         12.36         1.0  \n",
       "\n",
       "[80000 rows x 20 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
      "       'passenger_count', 'trip_distance', 'pickup_longitude',\n",
      "       'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude',\n",
      "       'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax',\n",
      "       'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
      "       'RateCodeID_Final'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "yellow = yellow.rename(columns={\n",
    "    'RateCodeID': 'RateCodeID_1',\n",
    "    'RatecodeID': 'RateCodeID_2'\n",
    "})\n",
    "yellow['RateCodeID_Final'] = yellow['RateCodeID_1'].combine_first(yellow['RateCodeID_2'])\n",
    "yellow = yellow.drop(['RateCodeID_1', 'RateCodeID_2'], axis=1)\n",
    "print(yellow.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "500dae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "print(yellow['VendorID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "14a9a747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
       "       '<=50k'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['age','workclass','fnlwgt','education','education-num',\n",
    "             'marital-status','occupation','relationship','race','sex',\n",
    "             'capital-gain','capital-loss','hours-per-week','native-country','<=50k']\n",
    "\n",
    "adult = pd.read_csv(datasets[\"adult\"], header=None, names=col_names)\n",
    "adult_test = pd.read_csv(datasets[\"adult_test\"], header=None, names=col_names, skiprows=1)\n",
    "\n",
    "adult = pd.concat([adult, adult_test], ignore_index=True)\n",
    "adult.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4da1d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K', ' <=50K.', ' >50K.'], dtype=object)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult['<=50k'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3b48d37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
       "       'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
       "       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
       "       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
       "       'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_churn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f996ae2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default.payment.next.month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCI_credit_card.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "049bbd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education-num      int64\n",
       "marital-status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital-gain       int64\n",
       "capital-loss       int64\n",
       "hours-per-week     int64\n",
       "native-country    object\n",
       "<=50k             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "565621c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerID           object\n",
       "gender               object\n",
       "SeniorCitizen         int64\n",
       "Partner              object\n",
       "Dependents           object\n",
       "tenure                int64\n",
       "PhoneService         object\n",
       "MultipleLines        object\n",
       "InternetService      object\n",
       "OnlineSecurity       object\n",
       "OnlineBackup         object\n",
       "DeviceProtection     object\n",
       "TechSupport          object\n",
       "StreamingTV          object\n",
       "StreamingMovies      object\n",
       "Contract             object\n",
       "PaperlessBilling     object\n",
       "PaymentMethod        object\n",
       "MonthlyCharges      float64\n",
       "TotalCharges         object\n",
       "Churn                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_churn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3c208834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                              int64\n",
       "LIMIT_BAL                     float64\n",
       "SEX                             int64\n",
       "EDUCATION                       int64\n",
       "MARRIAGE                        int64\n",
       "AGE                             int64\n",
       "PAY_0                           int64\n",
       "PAY_2                           int64\n",
       "PAY_3                           int64\n",
       "PAY_4                           int64\n",
       "PAY_5                           int64\n",
       "PAY_6                           int64\n",
       "BILL_AMT1                     float64\n",
       "BILL_AMT2                     float64\n",
       "BILL_AMT3                     float64\n",
       "BILL_AMT4                     float64\n",
       "BILL_AMT5                     float64\n",
       "BILL_AMT6                     float64\n",
       "PAY_AMT1                      float64\n",
       "PAY_AMT2                      float64\n",
       "PAY_AMT3                      float64\n",
       "PAY_AMT4                      float64\n",
       "PAY_AMT5                      float64\n",
       "PAY_AMT6                      float64\n",
       "default.payment.next.month      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCI_credit_card.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "fa26fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   int64\n",
       "trip_distance                   float64\n",
       "pickup_longitude                float64\n",
       "pickup_latitude                 float64\n",
       "store_and_fwd_flag               object\n",
       "dropoff_longitude               float64\n",
       "dropoff_latitude                float64\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "RateCodeID_Final                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow['tpep_dropoff_datetime']=pd.to_datetime(yellow['tpep_dropoff_datetime'])\n",
    "yellow['tpep_pickup_datetime']=pd.to_datetime(yellow['tpep_pickup_datetime'])\n",
    "\n",
    "yellow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee64719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "9bdf3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list=[yellow , UCI_credit_card , customer_churn,adult]\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "056be466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "def get_metrics(df_list,dataset_name_list):\n",
    "    data=[]\n",
    "    for df,dataset_name in zip(df_list,dataset_name_list):\n",
    "        df=pd.DataFrame(df)\n",
    "        #duplicate row percentage\n",
    "        duplicate_row_percentage=((len(df)-len(df.drop_duplicates()))/len(df))*100\n",
    "        for column in df.columns:\n",
    "            #Meta info-:\n",
    "            dataset_name=dataset_name\n",
    "            column_name=column\n",
    "            dtype=df[column].dtype\n",
    "        \n",
    "            #completness-:\n",
    "            non_null_count=df[column].notnull().sum()\n",
    "            missing_count=df[column].isnull().sum()\n",
    "            missing_percent=(missing_count/len(df[column]))*100\n",
    "        \n",
    "            #Uniquness and variety\n",
    "        \n",
    "            unique_count=df[column].nunique()\n",
    "            unique_percent=(unique_count/len(df[column]))*100\n",
    "            mode=df[column].mode()[0]\n",
    "            mode_percent=((df[column]==mode).sum()/len(df[column]))*100\n",
    "        \n",
    "        \n",
    "            #based on column types\n",
    "        \n",
    "            if is_numeric_dtype(df[column]):\n",
    "                mean=df[column].mean()\n",
    "                std=df[column].std()\n",
    "                min=df[column].min()\n",
    "                max=df[column].max()\n",
    "                median=df[column].median()\n",
    "            \n",
    "                scalar=StandardScaler()\n",
    "                z_score=scalar.fit_transform(df[[column]])\n",
    "                outliers=int((np.abs(z_score)>=3).sum())\n",
    "            \n",
    "                num_categories=None\n",
    "                rare_category_percent=None\n",
    "                whitespace_issues=None\n",
    "                min_date=None\n",
    "                max_date=None\n",
    "                date_range_days=None\n",
    "            elif((df[column].dtype=='object') or (df[column].dtype=='bool') or (df[column].dtype=='category') ):\n",
    "                num_categories=unique_count\n",
    "                unique=list(df[column].unique())\n",
    "                rare_category_percent=100\n",
    "                for item in unique:\n",
    "                    item_count=(df[column]==item).sum()\n",
    "                    item_percentage=(item_count/len(df[column]))*100\n",
    "                    if item_percentage<=5.5 and (item_percentage<rare_category_percent):\n",
    "                        rare_category_percent=item_percentage\n",
    "                if(rare_category_percent==100):\n",
    "                    rare_category_percent=None\n",
    "                temp1=df[column].str.startswith(\" \").sum()\n",
    "                temp2=df[column].str.endswith(\" \").sum()\n",
    "                whitespace_issues=temp1+temp2\n",
    "                mean=None\n",
    "                std=None\n",
    "                min=None\n",
    "                max=None\n",
    "                median=None\n",
    "                outliers=None\n",
    "                min_date=None\n",
    "                max_date=None\n",
    "                date_range_days=None\n",
    "            else:\n",
    "                min_date=df[column].min()\n",
    "                max_date=df[column].max()\n",
    "                if pd.notna(min_date) and pd.notna(max_date):\n",
    "                    date_range_days = (max_date - min_date).days\n",
    "                else:\n",
    "                    date_range_days = None\n",
    "                mean=None\n",
    "                std=None\n",
    "                min=None\n",
    "                max=None\n",
    "                median=None\n",
    "                outliers=None\n",
    "                num_categories=None\n",
    "                rare_category_percent=None\n",
    "                whitespace_issues=None\n",
    "            data.append([dataset_name,column_name,dtype,non_null_count,missing_count,missing_percent,unique_count,unique_percent,mode,mode_percent,mean,std,min,max,median,outliers,num_categories,rare_category_percent,whitespace_issues,min_date,max_date,date_range_days,duplicate_row_percentage])\n",
    "    return data        \n",
    "                    \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "cf44d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def get_probs(df, column , bins):\n",
    "    probabilities=[]\n",
    "    if is_numeric_dtype(df[column]):\n",
    "        bin_indices = pd.cut(\n",
    "        df[column],\n",
    "        bins=bins,\n",
    "        include_lowest=True,\n",
    "        labels=False\n",
    "        ) + 1\n",
    "        counter=Counter(bin_indices)\n",
    "        for i in range(1,len(bins)+1,1):\n",
    "            count=counter.get(i,0)\n",
    "            prob=(count)/(len(df[column])) #adding 1 for zero error handling\n",
    "            prob=max(prob,0.0000001)\n",
    "            probabilities.append(prob)\n",
    "    else:\n",
    "        counter=Counter(df[column])\n",
    "        for cat in bins:\n",
    "            count=counter.get(cat, 0)\n",
    "            prob=count / len(df[column])\n",
    "            prob=max(prob, 0.0000001)\n",
    "            probabilities.append(prob)\n",
    "    return probabilities\n",
    "\n",
    "def KL_PSI_and_JS(df_base, df_new , column ):\n",
    "        if is_numeric_dtype(df_base[column]):\n",
    "            if len(df_base[column])<10000:\n",
    "                bins = np.quantile(df_base[column], q=np.linspace(0,1,6))\n",
    "                bins = np.unique(bins)\n",
    "            elif len(df_base[column])<50000:\n",
    "                bins = np.quantile(df_base[column], q=np.linspace(0,1,11))\n",
    "                bins = np.unique(bins)\n",
    "            else:\n",
    "                bins = np.quantile(df_base[column], q=np.linspace(0,1,21))\n",
    "                bins = np.unique(bins)\n",
    "        else:\n",
    "            bins=df_base[column].unique()\n",
    "        probs_base=get_probs(df_base , column, bins)\n",
    "        probs_new=get_probs(df_new , column , bins)\n",
    "        KL_div=0\n",
    "        PSI_div=0\n",
    "        JS_div=0\n",
    "        probs_base=np.array(probs_base)\n",
    "        probs_new=np.array(probs_new)\n",
    "        M=(probs_new+probs_base)/2\n",
    "        for i in range(0,len(probs_base) , 1):\n",
    "            val_KL=probs_base[i]*(math.log(probs_base[i]/probs_new[i]))\n",
    "            val_PSI=(probs_base[i]-probs_new[i])*(math.log(probs_base[i]/probs_new[i]))\n",
    "            val_JS=(probs_base[i]*(math.log(probs_base[i]/M[i]))+probs_new[i]*(math.log(probs_new[i]/M[i])))/2\n",
    "            KL_div+=val_KL\n",
    "            PSI_div+=val_PSI\n",
    "            JS_div+=val_JS\n",
    "        return KL_div , PSI_div,JS_div\n",
    "    \n",
    "\n",
    "def get_entropy(df_base,column):\n",
    "    if is_numeric_dtype(df_base[column]):\n",
    "        if len(df_base[column])<10000:\n",
    "            bins = np.quantile(df_base[column], q=np.linspace(0,1,6))\n",
    "            bins = np.unique(bins)\n",
    "        elif len(df_base[column])<50000:\n",
    "            bins = np.quantile(df_base[column], q=np.linspace(0,1,11))\n",
    "            bins = np.unique(bins)\n",
    "        else:\n",
    "            bins = np.quantile(df_base[column], q=np.linspace(0,1,21))\n",
    "            bins = np.unique(bins)\n",
    "    else:\n",
    "        bins=df_base[column].unique()\n",
    "    probs=get_probs(df_base,column,bins)\n",
    "    entropy=0\n",
    "    for prob in probs:\n",
    "        entropy-=(prob*(math.log(prob)))\n",
    "    return entropy\n",
    "\n",
    "def get_type_error_count(df, column):\n",
    "    expected_type=df[column].dtype\n",
    "    error_count = 0\n",
    "    for val in df[column]:\n",
    "        if is_numeric_dtype(df[column]):\n",
    "            try:\n",
    "                float(val)  \n",
    "            except (ValueError, TypeError):\n",
    "                error_count += 1\n",
    "        elif expected_type == \"datetime64[ns]\":\n",
    "            try:\n",
    "                pd.to_datetime(val)\n",
    "            except (ValueError, TypeError):\n",
    "                error_count += 1\n",
    "        elif ((expected_type == \"category\") or (expected_type=='object')):\n",
    "            if not isinstance(val, str):\n",
    "                error_count += 1\n",
    "    return error_count\n",
    "\n",
    "def rule_based_errors(df,column,rules):\n",
    "    errors=0\n",
    "    rules=dict(rules)\n",
    "    if column in rules.keys():\n",
    "        for val in df[column]:\n",
    "            if not(rules[column](val)):\n",
    "                errors+=1\n",
    "        return errors\n",
    "    else:\n",
    "        return 0\n",
    "def check_age(val):\n",
    "    if val>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "rules={\n",
    "    \"age\": check_age\n",
    "    }    #column_name : function_name\n",
    "\n",
    "       \n",
    "def advanced_metrics(dataset_name_list,df_base_list,df_new_list):\n",
    "    metrics=[]\n",
    "    for df_base,df_new,dataset_name in zip(df_base_list,df_new_list,dataset_name_list):\n",
    "        df_base=pd.DataFrame(df_base)\n",
    "        df_new=pd.DataFrame(df_new)\n",
    "        for column in df_base.columns:\n",
    "            KL_div , PSI_div,JS_div =KL_PSI_and_JS(df_base , df_new,column)\n",
    "            entropy=get_entropy(df_base,column)\n",
    "            type_errors=get_type_error_count(df_base,column)\n",
    "            rule_errors=rule_based_errors(df_base,column,rules)\n",
    "        \n",
    "            #more adv metrics soon\n",
    "            metrics.append([dataset_name,column,KL_div,PSI_div,JS_div,entropy,type_errors,rule_errors])\n",
    "    return metrics\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "36f11ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_yellow=yellow['total_amount']\n",
    "# def convert_churn(value):\n",
    "#     if value=='Yes':\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# customer_churn['Churn']=customer_churn['Churn'].apply(convert_churn)\n",
    "# target_churn=customer_churn['Churn']\n",
    "# target_UCI=UCI_credit_card['default.payment.next.month']\n",
    "# def convert_adult(value):\n",
    "#     if (value=='>50K') or (value=='>50K.'):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# adult.columns = adult.columns.str.strip()\n",
    "# adult['<=50k']=adult['<=50k'].apply(convert_adult)\n",
    "# target_adult=adult['<=50k']\n",
    "# target_list=[target_yellow,target_UCI,target_churn,target_adult]\n",
    "# targets={}\n",
    "# for i , dataset in enumerate(dataset_list):\n",
    "#     targets[dataset_names[i]]=target_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a588e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def generate_meta(dataset_name_list,dataset_list):\n",
    "    df_old_list=[]\n",
    "    df_new_list=[]\n",
    "    data=get_metrics(dataset_list,dataset_name_list)\n",
    "    for df in dataset_list:\n",
    "        df_old , df_new =train_test_split(df,test_size=0.3,random_state=1)\n",
    "        df_old_list.append(df_old)\n",
    "        df_new_list.append(df_new)\n",
    "    metrics=advanced_metrics(dataset_name_list,df_old_list,df_new_list)\n",
    "    meta_data=pd.DataFrame(data=data,columns=[\n",
    "    # — Meta Info —\n",
    "    \"dataset_name\",        \n",
    "    \"column_name\",         \n",
    "    \"dtype\",               \n",
    "\n",
    "    # — Completeness —\n",
    "    \"non_null_count\",      \n",
    "    \"missing_count\",       \n",
    "    \"missing_percent\",     \n",
    "\n",
    "    # — Uniqueness & Variety —\n",
    "    \"unique_count\",        \n",
    "    \"unique_percent\",      \n",
    "    \"mode\",                \n",
    "    \"mode_percent\",        \n",
    "\n",
    "    # — Numeric Columns —\n",
    "    \"mean\",                \n",
    "    \"std\",                 \n",
    "    \"min\",                 \n",
    "    \"max\",                 \n",
    "    \"median\",              \n",
    "    \"outlier_count\",       \n",
    "\n",
    "    # — Categorical Columns —\n",
    "    \"num_categories\",      \n",
    "    \"rare_category_percent\",\n",
    "    \"whitespace_issues\",   \n",
    "\n",
    "    # — Datetime Columns —\n",
    "    \"min_date\",            \n",
    "    \"max_date\",            \n",
    "    \"date_range_days\",     \n",
    "\n",
    "    # — Quality & Resource —\n",
    "    \"duplicate_rows_percent\"    \n",
    "])\n",
    "    meta_data.replace({pd.NaT: np.nan}, inplace=True)\n",
    "    meta_table_adv=pd.DataFrame(data=metrics,columns=['dataset_name','column_name','KL_divergence','PSI_divergence','JS_divergence','Entropy','Type_Errors','rule_errors'])\n",
    "    meta_table_combined=pd.merge(meta_data,meta_table_adv,on=['dataset_name', 'column_name'],how='inner')\n",
    "    return meta_table_combined\n",
    "meta_table_combined=generate_meta(dataset_names,dataset_list) #step 1    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "32e8065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prabhat\\AppData\\Local\\Temp\\ipykernel_24408\\3135485019.py:2: FutureWarning: The provided callable <function nansum at 0x000002B24ABA70A0> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  summary = meta_table_combined.groupby(\"dataset_name\", as_index=False).agg({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>avg_missing_percent</th>\n",
       "      <th>avg_duplicate_percent</th>\n",
       "      <th>total_outliers</th>\n",
       "      <th>total_whitespace_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCI_credit_card</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7819.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adult</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059375</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>439578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_churn</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yellow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16929.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_name  avg_missing_percent  avg_duplicate_percent  \\\n",
       "0  UCI_credit_card                  0.0               0.000000   \n",
       "1            adult                  0.0               0.059375   \n",
       "2   customer_churn                  0.0               0.000000   \n",
       "3           yellow                  0.0               0.000000   \n",
       "\n",
       "   total_outliers  total_whitespace_issues  \n",
       "0          7819.0                      0.0  \n",
       "1          4250.0                 439578.0  \n",
       "2             0.0                     22.0  \n",
       "3         16929.0                      0.0  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#not rquired as much\n",
    "summary = meta_table_combined.groupby(\"dataset_name\", as_index=False).agg({\n",
    "    \"missing_percent\": \"mean\",\n",
    "    \"duplicate_rows_percent\": \"mean\",\n",
    "    \"outlier_count\": np.nansum,\n",
    "    \"whitespace_issues\": np.nansum\n",
    "})\n",
    "\n",
    "summary.rename(columns={\n",
    "    \"missing_percent\": \"avg_missing_percent\",\n",
    "    \"duplicate_rows_percent\": \"avg_duplicate_percent\",\n",
    "    \"outlier_count\": \"total_outliers\",\n",
    "    \"whitespace_issues\": \"total_whitespace_issues\"\n",
    "}, inplace=True)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9b79e853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset_name', 'column_name', 'dtype', 'non_null_count',\n",
       "       'missing_count', 'missing_percent', 'unique_count', 'unique_percent',\n",
       "       'mode', 'mode_percent', 'mean', 'std', 'min', 'max', 'median',\n",
       "       'outlier_count', 'num_categories', 'rare_category_percent',\n",
       "       'whitespace_issues', 'min_date', 'max_date', 'date_range_days',\n",
       "       'duplicate_rows_percent', 'KL_divergence', 'PSI_divergence',\n",
       "       'JS_divergence', 'Entropy', 'Type_Errors', 'rule_errors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_table_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "aa59bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_table_combined.to_csv('metrics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "babba5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow</td>\n",
       "      <td>VendorID</td>\n",
       "      <td>No major issues detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yellow</td>\n",
       "      <td>tpep_pickup_datetime</td>\n",
       "      <td>No major issues detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>tpep_dropoff_datetime</td>\n",
       "      <td>No major issues detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yellow</td>\n",
       "      <td>passenger_count</td>\n",
       "      <td>Handle outliers (cap/remove)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yellow</td>\n",
       "      <td>trip_distance</td>\n",
       "      <td>Handle outliers (cap/remove)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>adult</td>\n",
       "      <td>capital-gain</td>\n",
       "      <td>Handle outliers (cap/remove)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>adult</td>\n",
       "      <td>capital-loss</td>\n",
       "      <td>Handle outliers (cap/remove)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>adult</td>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>Handle outliers (cap/remove)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>adult</td>\n",
       "      <td>native-country</td>\n",
       "      <td>Strip leading/trailing spaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>adult</td>\n",
       "      <td>&lt;=50k</td>\n",
       "      <td>Strip leading/trailing spaces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_name            column_name                recommendations\n",
       "0        yellow               VendorID       No major issues detected\n",
       "1        yellow   tpep_pickup_datetime       No major issues detected\n",
       "2        yellow  tpep_dropoff_datetime       No major issues detected\n",
       "3        yellow        passenger_count   Handle outliers (cap/remove)\n",
       "4        yellow          trip_distance   Handle outliers (cap/remove)\n",
       "..          ...                    ...                            ...\n",
       "75        adult           capital-gain   Handle outliers (cap/remove)\n",
       "76        adult           capital-loss   Handle outliers (cap/remove)\n",
       "77        adult         hours-per-week   Handle outliers (cap/remove)\n",
       "78        adult         native-country  Strip leading/trailing spaces\n",
       "79        adult                  <=50k  Strip leading/trailing spaces\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_recommendations(meta_data_combined):\n",
    "    recs = []\n",
    "    for i, row in meta_data_combined.iterrows():\n",
    "        suggestions = []\n",
    "        if row['missing_percent']>20:\n",
    "            suggestions.append(\"Consider dropping column due to high missing values\")\n",
    "        elif 0 < row['missing_percent'] <= 20:\n",
    "            suggestions.append(\"Impute missing values (mean/median/mode)\")\n",
    "        if row['outlier_count'] and row['outlier_count']>0:\n",
    "            suggestions.append(\"Handle outliers (cap/remove)\")\n",
    "        if row['whitespace_issues'] and row['whitespace_issues']>0:\n",
    "            suggestions.append(\"Strip leading/trailing spaces\")\n",
    "        if row['rare_category_percent'] and row['rare_category_percent']>5:\n",
    "            suggestions.append(\"Combine rare categories or resolve typos\")\n",
    "        if row['dtype'] == 'datetime64[ns]' and row['date_range_days'] < 1:\n",
    "            suggestions.append(\"Check for incorrect or same date entries\")\n",
    "        if row['Type_Errors']>0:\n",
    "            suggestions.append(\"Missmatch type detected\")\n",
    "        if row['rule_errors']>0:\n",
    "            suggestions.append(\"Buisness rule violated\")\n",
    "        if len(suggestions) == 0:\n",
    "            suggestions.append(\"No major issues detected\")\n",
    "        recs.append({\n",
    "            \"dataset_name\": row[\"dataset_name\"],\n",
    "            \"column_name\": row[\"column_name\"],\n",
    "            \"recommendations\": \", \".join(suggestions)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(recs)\n",
    "recommendations=cleaning_recommendations(meta_table_combined)#step 2\n",
    "recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "6460a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import ngrams\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "\n",
    "def clean(df, column, clean_type):\n",
    "    def clean_outliers(df, column):\n",
    "        while True:\n",
    "            col = df[column]\n",
    "            if col.std() == 0:\n",
    "                break\n",
    "            z = ((col - col.mean()) / col.std()).abs()\n",
    "            outlier_idx = z >= 3\n",
    "            if outlier_idx.sum() == 0:\n",
    "                break\n",
    "            df.loc[outlier_idx, column] = col.median()\n",
    "        return df[column]\n",
    "\n",
    "    def keep_string_else_none(val):\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        else:\n",
    "            return None\n",
    "        #1 Drop column due to high missing values\n",
    "    if clean_type == 1:\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "        return None\n",
    "        #2 Impute missing values\n",
    "    elif clean_type == 2:\n",
    "        if (is_numeric_dtype(df[column]) and (df[column].dtype != 'bool')) :\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        elif df[column].dtype=='datetime64[ns]':\n",
    "            df[column]=df[column].fillna(df[column].mode()[0])\n",
    "        else:\n",
    "            df[column]=df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "        #3 Handle outliers using Z-score method\n",
    "    elif clean_type == 3:\n",
    "        df[column] = clean_outliers(df, column)  \n",
    "        #4 Strip leading/trailing spaces\n",
    "    elif clean_type == 4:\n",
    "        def striping(value):\n",
    "            if pd.isna(value):\n",
    "                return value\n",
    "            return str(value).strip()\n",
    "        df[column] = df[column].apply(striping)\n",
    "\n",
    "        #5 Combine rare categories and resolve typos\n",
    "    elif clean_type == 5:\n",
    "        condition=False\n",
    "        breakpoint=0\n",
    "        substitutes={}\n",
    "        if df[column].dtype == 'object' or df[column].dtype == 'category':\n",
    "            series=df[column].value_counts()\n",
    "            for i,(k,v) in enumerate(series.items()):\n",
    "                v=(v/len(df[column]))*100\n",
    "                if v<=5:#checking if its a typo\n",
    "                    if not (condition):\n",
    "                        breakpoint=i\n",
    "                        condition=True\n",
    "                    if len(str(k))>=3:\n",
    "                        k_ngrams=ngrams(str(k).lower(),3)\n",
    "                        k_set=set()\n",
    "                        for gram in k_ngrams:\n",
    "                            k_set.add(\" \".join(gram))\n",
    "                        \n",
    "                        for index,key in enumerate(series.index):\n",
    "                            if index==breakpoint:\n",
    "                                break\n",
    "                            key_ngrams=ngrams(str(key).lower(),3)\n",
    "                            key_set=set()\n",
    "                        \n",
    "                            for gram in key_ngrams:\n",
    "                               key_set.add(\" \".join(gram))\n",
    "                            if len(k_set.union(key_set)) == 0:\n",
    "                                distance = 1\n",
    "                            else:\n",
    "                                distance = jaccard_distance(k_set, key_set)\n",
    "                            if distance<0.25:\n",
    "                                substitutes[k]=key\n",
    "                            else:\n",
    "                                substitutes[k]='Other'#rare category \n",
    "            df[column]=df[column].replace(substitutes)               \n",
    "        #6 Check for incorrect/same date entries\n",
    "    elif clean_type == 6:\n",
    "        if ((df[column].dtype=='datetime64[ns]') and (df[column].nunique()<=1)):\n",
    "            print( column , \"has identical or invalid date entries not required for data analysis\")\n",
    "    #7 resolve missmatches and convert to nan\n",
    "    elif clean_type == 7:\n",
    "        expected_type = df[column].dtype\n",
    "        if is_numeric_dtype(expected_type):\n",
    "            df[column]=pd.to_numeric(df[column], errors='coerce')\n",
    "        elif expected_type=='datetime64[ns]':\n",
    "            df[column]=pd.to_datetime(df[column], errors='coerce')\n",
    "        elif expected_type in ['object', 'category']:\n",
    "            df[column]=df[column].apply(keep_string_else_none)\n",
    "            \n",
    "    #8 convert invalid buisness value to nan (avoiding hallucinating of values)\n",
    "    elif clean_type==8:\n",
    "        func=rules[column]\n",
    "        for i,val in enumerate(df[column]):\n",
    "            if not(func(val)):\n",
    "                df.loc[i,column]=None\n",
    "              \n",
    "    elif clean_type == 9:\n",
    "        pass\n",
    "    return df[column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0c6d3aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prabhat\\AppData\\Local\\Temp\\ipykernel_24408\\577621692.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '178144.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[outlier_idx, column] = col.median()\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dataset_list=[yellow , UCI_credit_card , customer_churn,adult]\n",
    "def get_cleaned_data(dataset_list,recommendations):\n",
    "    mapped_suggestions={\n",
    "        1:\"Consider dropping column due to high missing values\" ,\n",
    "        2:\"Impute missing values (mean/median/mode)\",\n",
    "        3:\"Handle outliers (cap/remove)\",\n",
    "        4:\"Strip leading/trailing spaces\",\n",
    "        5:\"Combine rare categories or resolve typos\",\n",
    "        6:\"Check for incorrect or same date entries\",\n",
    "        7:\"Missmatch type detected\",\n",
    "        8:\"Buisness rule violated\",\n",
    "        9:\"No major issues detected\"\n",
    "    }\n",
    "    dataset_list_cleaned=[]\n",
    "    for item in dataset_list:\n",
    "        item=item.copy(deep=True)\n",
    "        for column in item.columns:\n",
    "            suggesions=recommendations[recommendations['column_name']==column]['recommendations'].iloc[0]\n",
    "            suggesions=str(suggesions).split(',')\n",
    "            #print(\"would you like to perform the following cleaning operations on \"+str(column),'-:' ,'\\n',suggesions,'\\n','please answer Yes or No')\n",
    "            #ans=input()\n",
    "            #ans=ans.lower()\n",
    "            #if(ans=='no'):\n",
    "            #    continue\n",
    "            for suggesion in suggesions:\n",
    "                suggesion=str(suggesion).strip()\n",
    "                for k,v in mapped_suggestions.items():\n",
    "                    v=v.strip()\n",
    "                    if suggesion==v:\n",
    "                        clean_type=k\n",
    "                        break\n",
    "                result=clean(item, column, clean_type)\n",
    "                if result is not None:\n",
    "                    item[column] = result\n",
    "        item=item.drop_duplicates()\n",
    "        dataset_list_cleaned.append(item)\n",
    "    return dataset_list_cleaned  \n",
    "dataset_list_cleaned=get_cleaned_data(dataset_list,recommendations) #step 3             \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "678f1e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48805"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_list_cleaned[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "58d7f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_table_combined_cleaned=generate_meta(dataset_names,dataset_list_cleaned)# step 4 optional  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f3bdd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_tables=[meta_table_combined,meta_table_combined_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "34ebb3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_table_combined_cleaned['outlier_count'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5c3d5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_for_DQ_computation(meta_table):\n",
    "    summarized_meta=meta_table.drop(columns=['dtype','non_null_count','missing_count','unique_count','mode','mode_percent','mean','std','min','max','median','num_categories','min_date','max_date','date_range_days','Entropy'])\n",
    "    table_lengths={}\n",
    "    for index,data in enumerate(dataset_list):\n",
    "        table_lengths[dataset_names[index]]=len(data)\n",
    "    outlier_percent=[]\n",
    "    whitespace_percent=[]\n",
    "    rule_errors_percent=[]\n",
    "    for index,val in summarized_meta.iterrows():\n",
    "        outlier_percent.append((val['outlier_count']/table_lengths[val['dataset_name']])*100)\n",
    "        whitespace_percent.append((val['whitespace_issues']/table_lengths[val['dataset_name']])*100)\n",
    "        rule_errors_percent.append((val['rule_errors']/table_lengths[val['dataset_name']])*100)\n",
    "    summarized_meta['outlier_percent']=outlier_percent\n",
    "    summarized_meta['whitespace_percent']=whitespace_percent\n",
    "    summarized_meta['rule_errors_percent']=rule_errors_percent\n",
    "    summarized_meta.drop(columns=['outlier_count','whitespace_issues','rule_errors','Type_Errors','unique_percent','rare_category_percent','KL_divergence'],inplace=True)\n",
    "    return summarized_meta\n",
    "summarized_meta_table=get_table_for_DQ_computation(meta_table_combined)#step 5\n",
    "summarized_meta_table_cleaned=get_table_for_DQ_computation(meta_table_combined_cleaned)#step 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ee4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "700958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weights(dataset_name,summarized_meta):\n",
    "    metrics=list(summarized_meta.columns)\n",
    "    metrics.remove('dataset_name')\n",
    "    metrics.remove('column_name')\n",
    "    #while True:\n",
    "        #print(f'enter your weights for each metric for {dataset_name} , they must sum to 1 , else metrics wont be considered\\n')\n",
    "    weights=[1/len(metrics)]*len(metrics)\n",
    "        # for i,metric in enumerate(metrics):\n",
    "        #     print(\"enter the weight for the following metric\\n\",metric+'\\n','default value is ',1/len(metrics),'\\n','press enter for default\\n')\n",
    "        #     weight=input()\n",
    "        #     weight=weight.strip()\n",
    "        #     if weight == \"\":\n",
    "        #         continue\n",
    "        #     elif weight.replace('.', '', 1).isdigit():\n",
    "        #         weights[i] = float(weight)\n",
    "        #     else:\n",
    "        #         print(\"Invalid input, keeping default.\\n\")\n",
    "        # weight_sum=sum(weights)\n",
    "        # if abs(weight_sum - 1) < 1e-6:\n",
    "        #     print(\"weights accepted\")\n",
    "        #     return weights\n",
    "        #     break\n",
    "        # else:\n",
    "        #     print('weights not accepted , sum not 100 , please try again\\n')\n",
    "    return weights\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "53a3e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "def get_normalized_divergence(temp,metric):\n",
    "    if metric=='PSI_divergence':\n",
    "        vals=[]\n",
    "        for index,row in temp.iterrows():\n",
    "            value=row[metric]\n",
    "            if(value<0.1):\n",
    "                vals.append(1)\n",
    "            elif(value>0.1 and value<0.25):\n",
    "                vals.append(0.4)\n",
    "            else:\n",
    "                vals.append(0)\n",
    "        temp[metric]=vals\n",
    "        return temp[metric]\n",
    "    else:\n",
    "        js_penalties = []\n",
    "        for dataset, group in temp.groupby(\"dataset_name\",sort=False):\n",
    "            js_vals = group[metric].values\n",
    "            js_percentiles = [\n",
    "                percentileofscore(js_vals, v, kind=\"weak\") / 100\n",
    "                for v in js_vals\n",
    "                ]\n",
    "            for js_val, js_pct in zip(js_vals, js_percentiles):\n",
    "                if js_val < 0.01:          # negligible JS\n",
    "                    js_penalties.append(1)\n",
    "                else:\n",
    "                    js_penalties.append(1-js_pct)\n",
    "        temp[metric]=js_penalties\n",
    "    return temp[metric]\n",
    "                        \n",
    "def get_new_values(summarized_meta_table):\n",
    "    metrics=list(summarized_meta_table.columns)\n",
    "    metrics.remove('dataset_name')\n",
    "    metrics.remove('column_name')\n",
    "    temp=summarized_meta_table.copy(deep=True)\n",
    "    for i,metric in enumerate(metrics):\n",
    "        if (metric=='PSI_divergence') or (metric=='JS_divergence'):\n",
    "            temp[metric]=get_normalized_divergence(temp,metric)\n",
    "        else:\n",
    "            vals=[]\n",
    "            for index,row in temp.iterrows():\n",
    "                if((row[metric]<1)or(pd.isna(row[metric]))):\n",
    "                    vals.append(1)\n",
    "                elif(row[metric]<5):\n",
    "                    vals.append(0.8)\n",
    "                elif(row[metric]<10):\n",
    "                    vals.append(0.4)\n",
    "                else:\n",
    "                    vals.append(0)\n",
    "            temp[metric]=vals\n",
    "    return temp\n",
    "    \n",
    "def Compute_DQ_Score(dataset_name_list,dataset_list,summarized_meta):\n",
    "    metrics=list(summarized_meta.columns)\n",
    "    metrics.remove('dataset_name')\n",
    "    metrics.remove('column_name')\n",
    "    temp=get_new_values(summarized_meta)\n",
    "    scores={}\n",
    "    for dataset_name,df in zip(dataset_name_list,dataset_list):\n",
    "        weights=get_weights(dataset_name,summarized_meta)\n",
    "        DQ_Score=[]\n",
    "        for column in df.columns:\n",
    "            value=0\n",
    "            val=0\n",
    "            for j,metric in enumerate(metrics):\n",
    "                value=temp.loc[temp['column_name']==column,metric].values[0]*weights[j]\n",
    "                if pd.isna(value):\n",
    "                    val += weights[j]\n",
    "                else:\n",
    "                    val+=min(value,weights[j])\n",
    "            DQ_Score.append(val)\n",
    "        DQ_Score=np.array(DQ_Score)\n",
    "        DQ_Score=np.mean(DQ_Score)*100\n",
    "        scores[dataset_name]=float(DQ_Score)\n",
    "    return scores\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "3729c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores before cleaning\n",
      "\n",
      "{'yellow': 95.52829442026116, 'UCI_credit_card': 98.51428571428569, 'customer_churn': 97.31130547457076, 'adult': 90.85714285714288}\n",
      "scores after cleaning\n",
      "\n",
      "{'yellow': 97.0320538187574, 'UCI_credit_card': 99.99999999999997, 'customer_churn': 97.31130547457076, 'adult': 99.99999999999999}\n"
     ]
    }
   ],
   "source": [
    "print(\"scores before cleaning\\n\")\n",
    "dirty_scores=Compute_DQ_Score(dataset_names,dataset_list,summarized_meta_table)#step 7 \n",
    "print(dirty_scores)\n",
    "print(\"scores after cleaning\\n\")\n",
    "cleaned_scores=Compute_DQ_Score(dataset_names,dataset_list_cleaned,summarized_meta_table_cleaned)#step 8\n",
    "print(cleaned_scores)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e5525ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('yellow', 95.52829442026116), ('UCI_credit_card', 98.51428571428569), ('customer_churn', 97.31130547457076), ('adult', 90.85714285714288)])\n"
     ]
    }
   ],
   "source": [
    "print(dirty_scores.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prabhat\\AppData\\Local\\Temp\\ipykernel_24408\\577621692.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '178144.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[outlier_idx, column] = col.median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores before cleaning\n",
      "\n",
      "{'yellow': 95.52829442026116, 'UCI_credit_card': 98.51428571428569, 'customer_churn': 97.31130547457076, 'adult': 90.85714285714288}\n",
      "scores after cleaning\n",
      "\n",
      "{'yellow': 97.0320538187574, 'UCI_credit_card': 99.99999999999997, 'customer_churn': 97.31130547457076, 'adult': 99.99999999999999}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'yellow': 95.52829442026116,\n",
       " 'UCI_credit_card': 98.51428571428569,\n",
       " 'customer_churn': 97.31130547457076,\n",
       " 'adult': 90.85714285714288}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy \n",
    "def Run_DQ_Pipeline():\n",
    "    meta_table_combined=generate_meta(dataset_names,dataset_list) #step 1   \n",
    "    recommendations=cleaning_recommendations(meta_table_combined)#step 2\n",
    "    dataset_list_cleaned=get_cleaned_data(dataset_list,recommendations) #step 3             \n",
    "    meta_table_combined_cleaned=generate_meta(dataset_names,dataset_list_cleaned)# step 4 optional  \n",
    "    summarized_meta_table=get_table_for_DQ_computation(meta_table_combined)#step 5\n",
    "    summarized_meta_table_cleaned=get_table_for_DQ_computation(meta_table_combined_cleaned)#step 6\n",
    "    print(\"scores before cleaning\\n\")\n",
    "    dirty_scores=Compute_DQ_Score(dataset_names,dataset_list,summarized_meta_table)#step 7 \n",
    "    print(dirty_scores)\n",
    "    print(\"scores after cleaning\\n\")\n",
    "    cleaned_scores=Compute_DQ_Score(dataset_names,dataset_list_cleaned,summarized_meta_table_cleaned)#step 8\n",
    "    print(cleaned_scores)  \n",
    "    return dirty_scores, cleaned_scores\n",
    "scores,scores_cleaned=Run_DQ_Pipeline()\n",
    "scores=pd.DataFrame(scores.items(),columns=['Dataset_name','DQ_Score'])\n",
    "scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9fb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
